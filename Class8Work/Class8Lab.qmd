---
title: "Class8: Mini Project"
author: "Xinyu Wen (A17115443)"
format: pdf
toc: TRUE
---

Today we will do a complete analysis of some breast cancer biopsy data, but first,  let's revisit the main PCA functions in R, `prcomp()`, and see what `scale=TRUE/FALSE` does.

```{r}
head(mtcars)
```

Find the mean value per column of this dataset.

```{r}
apply(mtcars, 2, mean)
```
```{r}
apply(mtcars, 2, sd)
```

It is clear that "disp" and "hp" have the highest mean values and the highest standard deviation. They will dominate any analysis we do on this dataset. Let's see. 

```{r}
pc.noscale <- prcomp(mtcars, scale = F)
pc.scale <- prcomp(mtcars, scale = T)
```

```{r}
biplot(pc.noscale)
```

```{r}
pc.noscale$rotation[,1]
```

Plot the loadings
```{r}
library(ggplot2)
r1 <- as.data.frame(pc.noscale$rotation)
r1$names <- rownames(pc.noscale$rotation)

ggplot(r1)+
    aes(PC1, names) +
    geom_col()
```

```{r}
r2 <- as.data.frame(pc.scale$rotation)
r2$names <- rownames(pc.scale$rotation)

ggplot(r2)+
    aes(PC2, names) +
    geom_col()
```

```{r}
biplot(pc.scale)
```

> **Take-home**: Generally we always want to set `scale-TRUE` when we do this type of analysis to avoid our analysis being dominated by individual varibles with the largest variance just due to their unit of measurement.


# 1. Exploratory data analysis - FNA breast cancer data

Load the data into R.

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.df)
```
> Q2. How many of the observations have a malignant diagnosis?

The table function is really useful here.

```{r}
table(wisc.df$diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
ncol(wisc.df)
```
```{r}
colnames(wisc.df)
```

A useful function for this is `grep()`
```{r}
length(grep("_mean", colnames(wisc.df)))

```

# 2. Principal Component Analysis
## Exclude diagnosis

Before we go any further, we need to exclude the diagnosis column from any future analysis - this tells us whether a sample is cancer or non-cancer.


```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
head(diagnosis)
```

```{r}
wisc.data <- wisc.df[,-1]
```

Let's see if we can cluster the `wisc.data` to find some structure in the dataset.

```{r}
hc <- hclust( dist(wisc.data))
plot (hc)
```

## Principal Component Analysis (PCA)

```{r}
wisc.pr <- prcomp( wisc.data, scale = T)
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
sum(0.4427, 0.1897, 0.09393)
```
PC1 to PC3

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

```{r}
sum(0.4427, 0.1897, 0.09393, 0.06602, 0.05496, 0.04025, 0.02251)
```
PC1 to PC7


```{r}
biplot(wisc.pr)
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

All the arrows are pointing toward the left or pointing down on the plot.
This biplot is too messy. The letters and numbers overlapping makes it too hard to interpret.

```{r}
head(wisc.pr$x)
```

Plot of PC1 vs PC2, the first 2 columns.

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

Make a ggplot version of this score plot, PC2 vs PC1.

```{r}
pc <- as.data.frame(wisc.pr$x)

ggplot(pc) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
pc <- as.data.frame(wisc.pr$x)

ggplot(pc) +
  aes(PC1, PC3, col=diagnosis) +
  geom_point()
```
Both of the plots have similar scattering pattern for the benign and malignant clusters. For PC1 vs PC2, there is a more clear separation between the clusters. For PC1 vs PC3, there is more overlapping at the verge of two clusters.



### Variance explained

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
#variance explained by each principal component / total variance explained of all principal components
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```



> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?


```{r}
wisc.pr$rotation["concave.points_mean", 1]
```


> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr)
```
```{r}
sum(0.4427, 0.1897, 0.09393, 0.06602, 0.05496)
```
PC1 to PC5



# 3. Hierarchical Clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
#Calculate the (Euclidean) distances
data.dist <- dist(data.scaled)
```

```{r}
#Create a hierarchical clustering model
wisc.hclust <- hclust(data.dist, method = "complete")
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19.5, col="red", lty=2)
```
height is about 19.5


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k= 4)
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

No. Any other cluster amount between 2 and 10 cannot give a cluster with more than 343 benign in a cluster or more than 165 malignant in a cluster.

## Clustering in PC space

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
#single
hc1 <- hclust(dist(wisc.pr$x[,1:2]), method = "single")
plot(hc1)
```

```{r}
#complete
hc2 <- hclust(dist(wisc.pr$x[,1:2]), method = "complete")
plot(hc2)
```

```{r}
#average
hc3 <- hclust(dist(wisc.pr$x[,1:2]), method = "average")
plot(hc3)
```

```{r}
#ward.D2
hc <- hclust(dist(wisc.pr$x[,1:2]), method = "ward.D2")

plot(hc)
abline(h=70, col="red")
```

I like "ward.D2" the most, because the branches are more symmetrical, instead of more compacted on the left side like the others. It is thus easier to read.


# 4. OPTIONAL: K-means clustering

```{r}
wisc.km <- kmeans(wisc.data, centers= 2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```

> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

```{r}
# hclust results
table(wisc.hclust.clusters, wisc.km$cluster)
```

I think k-means separated the diagnoses well. It results in 2 clusters, corresponding to the two types of diagnoses. Majority of benign is in one cluster, and majority of malignant is in the other cluster. Compared to the hclust results, k-means clustering is better. Hclust results have 4 clusters, which do not match the diagnoses groups that well.


# 5. Combining methods
## Cluster PC1~7

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")

plot(wisc.pr.hclust)
```
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```
These two ways of plotting have opposite color for each group. To make is consistent, we can convert the data into factor:
```{r}
g <- as.factor(grps)
levels(g)

g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```
Now the red is malignant, black is benign.











```{r}
wisc.pr.hclust.clusters.4 <- cutree(wisc.pr.hclust, k=4)
table(wisc.pr.hclust.clusters.4, diagnosis)
```

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
# Compare to actual diagnoses
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters)
```

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

If the new model separate the two diagnoses into 4 clusters, the malignant is spread out into four clusters, instead of grouping the majority into one cluster. Most of the benign is still in one cluster. Compare to the actual dianoses, the new model is worse.


> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.


```{r}
# k-means
table(wisc.km$cluster, diagnosis)
```

```{r}
# hclust results
table(wisc.hclust.clusters, diagnosis)
```

K-means separate the diagnoses well, as the number of clusters match the number of diagnoses groups. Hierarchical clustering separate the dignoses into 4 clusters. Thus is does not work as well as k-means.

# 6. Sensitivity/Specificity

## Hierarchical clustering
```{r}
grps1 <- cutree(hc, h=70) #hc from Q13, hc <- hclust(dist(wisc.pr$x[,1:2]), method = "ward.D2")
table(grps1)
```

```{r}
#same in Q2
table(diagnosis)
```

Crosstable to see how my clustering groups correspond to the expert diagnosis vector of M an B values.

```{r}
table(grps1, diagnosis)
```

Positive => Malignant, Cancer
Negative => Benign, non-cancer

True => cluster group 1
False => cluster grp 2

True Positive = 177 (want to optimize) 
False positive = 18 (minimize)

True Negative = 339 (want to optimize) (sensitivity) 
False Negative = 35 (minimize)

```{r}
Sensitivity = 177/(177+35)
Sensitivity

Specificity = 339/(339+18)
Specificity
```


## K-means
```{r}
table(wisc.km$cluster, diagnosis)
```
True Positive =  130
False positive =  1

True Negative =  356
False Negative =  82

```{r}
Sensitivity = 130/(130+82)
Sensitivity

Specificity = 356/(356+1)
Specificity
```

## Combining PCA & hierarchical
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```
True Positive =  188
False positive =  28

True Negative =  329
False Negative =  24

```{r}
Sensitivity = 188/(188+24)
Sensitivity

Specificity = 329/(329+28)
Specificity
```

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

Combining PCA and hierarchical clustering gives the highest sensitivity. K-means gives the best specificity.

# 7. Prediction
We can use our PCA results (wisc.pr) to make predictions on new unseen data.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col = g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

Patient 1 is clustered with benign. Patient 2 is clustered with malignant. Thus we should prioritize patient 2.

