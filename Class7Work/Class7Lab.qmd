---
title: "Class 7: Intro to Machine Learning"
author: "Xinyu Wen (A17115443)"
format: pdf
toc: True
---

Today we will explore unsupervised machine learning methods including clustering and dimentionality reduction methods. 

Let's start by making up some data (where we know there rae clear groups) that we can use to test out different clustering methods.

We can use the `rnorm()` for 

```{r}
hist(rnorm(n=3000, mean = 3))
```

Make data `z` with two "clusters"

```{r}
rnorm(30, mean=-3)
rnorm(30, mean= +3)
```

```{r}
x <- c(rnorm(30, mean=-3),
rnorm(30, mean= +3))

hist(x)
```

```{r}
x <- c(rnorm(30, mean=-3),
rnorm(30, mean= +3))

z <- cbind(x=x, y=rev(x))
head(z)

plot(z)
```
How big is `z`
```{r}
nrow(z)
ncol(z)
```



## K-means clustering
The main function in "base" R for K-means clustering is called `kmeans()`.

```{r}
k <- kmeans(z, centers = 2)
k
```
```{r}
attributes(k)
```

> Q. How many points lie in each cluster? (aka. size of cluster?)

```{r}
k$size
```


> Q. What component of our results tells us about the cluster membership (i.e. which point likes in which cluster)?

```{r}
k$cluster
```

> Q. Center of each cluster?

```{r}
k$centers
```

> Q. Put this result info together and make a little "base R" plot of our clustering result. Also add the cluster center points to this plot.

```{r}
plot(z, col = "blue")
```

```{r}
plot(z, col=c(1, 2))
```

Plot colored by cluster membership:

```{r}
plot(z, col=k$cluster)
points(k$centers, col="blue", pch = 15)
```

> Q. Run kmeans on our input `z` and define 4 clusters making the same result vizualization plot as above (plot of z colored by cluster membership).

```{r}
k4 <- kmeans(z, center = 4)

plot(z, col=k4$cluster)
points(k$centers, col="purple", pch = 15)
```


## Hierarchical Clustering

The main function in base R for this is called `hclust()`. It will take as input a distance matrix (key point is that you can't just give your "row" data as input. You have to first calculate a distance matrix from your data).

```{r}
d <- dist(z)
hc <- hclust(d)
hc
```

```{r}
plot(hc)
abline(h=10, col = "red")
```

Once I inspect the "tree" of dendrogram, I can "cut" the tree to yield my groupings or clusters. The function to this is called `cutree()`.

```{r}
grps <- cutree(hc, h=10)
```

```{r}
plot(z, col = grps)
```

# 1. PCA of UK food data

## Hands on with Principal Component Analysis (PCA)

Let's examine a 17-dimensional data containing details of food consumption in the UK. Are these countries different? How?


### Data import
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names =1) #Q2, first way
x
```


> Q1. How many rows and columns are in your new data frame named X? What R functions could you use to examine this?

```{r}
nrow(x)
ncol(x)
dim(x)
```
> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

```{r}
#The second way
#rownames(x) <- x[,1]
#x <- x[,-1]
#head(x)
```

I like the first way better, because it eliminates first column and read the file in one line. However, if I do not know which column of the file I should eliminate, it would be better to print the original table first, then use the second way to eliminate the column.

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
> Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```
Set beside as `False`.

> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(nrow(x)), pch =16) 
```
If a point lies on the diagonal of a plot, it means the two countries have similar comsumption of that food. 


> Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The main difference is that N. Ireland consumes much more fresh potatoes.



Looking at these types of "pairwise plots" can be helpful but it does not scale well. There must be a better way......

## PCA to the rescue!

The main function for PCA in base R is called `prcomp()`. This function wants the transpose of our input data - ie the important foods in as columns and the countries as rows.

```{r}
pca <- prcomp( t(x) )
summary(pca)
```


Let's see what is in our PCA result object `pca`.

```{r}
head(pca$x)
```


The `pca$x` result object is where we will focus first as this details how the countries are. related to each other in terms of our new "axis" (aka. "PCs", "eigenvectors", etc.)

> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

## Plot PC1 vs PC2
```{r}
plot(pca$x[,1], pca$x[,2], col=c("orange", "red", "blue", "darkgreen"), pch =16, xlab="PC1", ylab="PC2", xlim=c(-270,500))
```
```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```
> Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange", "red", "blue", "darkgreen"))
```


We can look at the so called PC "loadings" result object to see how the original foods contribute to our new PCs. (ie, how the original variables contribute to our new better PC variable)
```{r}
pca$rotation[,1]
```




```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```
```{r}
## or the second row here...
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```
> Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
#Loading plot for PC2
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```
Soft drinks and fresh potatoes are the two prominent food groups. PC2 tells us that the consumption of fresh potatoes and soft drinks have opposite trends. If fresh potatoes are larges consumed, soft drinks are not. And vise versa.

# 2. PCA of RNA-seq data
```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

> Q10. How many genes and samples are in this data set?


```{r}
genes <- nrow(rna.data)
genes
samples <- ncol(rna.data)
samples
```

There are 100 genes and 10 samples.



```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un-polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```
```{r}
summary(pca)
```
```{r}
plot(pca, main="Quick scree plot")
```

```{r}
## Variance captured per PC 
pca.var <- pca$sdev^2

## Percent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per

barplot(pca.var.per, main="Scree Plot", 
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```

### ggplot
```{r}
library(ggplot2)

df <- as.data.frame(pca$x)

# Our first basic plot
ggplot(df) + 
  aes(PC1, PC2) + 
  geom_point()
```
```{r}
# Add a 'wt' and 'ko' "condition" column
df$samples <- colnames(rna.data) 
df$condition <- substr(colnames(rna.data),1,2)

p <- ggplot(df) + 
        aes(PC1, PC2, label=samples, col=condition) + 
        geom_label(show.legend = FALSE)
p
```
```{r}
p + labs(title="PCA of RNASeq Data",
       subtitle = "PC1 clealy seperates wild-type from knock-out samples",
       x=paste0("PC1 (", pca.var.per[1], "%)"),
       y=paste0("PC2 (", pca.var.per[2], "%)"),
       caption="Class example data") +
     theme_bw()
```

## Optional: Gene loadings
```{r}
loading_scores <- pca$rotation[,1]

## Find the top 10 measurements (genes) that contribute
## most to PC1 in either direction (+ or -)
gene_scores <- abs(loading_scores) 
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)

## show the names of the top 10 genes
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes 
```








